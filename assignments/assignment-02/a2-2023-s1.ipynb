{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ffa401",
   "metadata": {},
   "source": [
    "# Assignment 2: Classification and Evaluation (20 marks)\n",
    "\n",
    "Student Name: Jiahao Shen\n",
    "\n",
    "Student ID: 1381187\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: *5 pm on Friday 7th of April*\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count)\n",
    "<ul>\n",
    "    <li>one day late, -2;</li>\n",
    "    <li>two days late, -4;</li>\n",
    "    <li>three days late, -6;</li>\n",
    "    <li>four days late, -8;</li>\n",
    "    <li>five days late, -10;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Marks</b>: This assignment will be marked out of 20, and make up 20% of your overall mark for this subject.\n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page] on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages `numpy`, `pandas`, `matplotlib` and `sklearn`. You can use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on Canvas>Assignments>Assignmnet1; we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: This assignment is an individual task, and so reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/124196/modules#module_662096\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where collusion or plagiarism are deemed to have taken place. Content produced by an AI (including, but not limited to ChatGPT) is not your own work, and submitting such content will be treated as a case of academic misconduct, in line with the <a href=\"https://academicintegrity.unimelb.edu.au/plagiarism-and-collusion/artificial-intelligence-tools-and-technologies\"> University's policy</a>.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>Missing Authorship Declaration at the bottom of the page, -2.0\n",
    "<LI>Incomplete or unsigned Authorship Declaration at the bottom of the page, -1.0\n",
    "</UL>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d47f4",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "For this assignment, you will work with a provided dataset to train and utilize multiple classifiers to analyze different versions of the dataset. In addition to implementing these classifiers, you will also explore various evaluation paradigms and analyze the impact of multiple parameters on the performance of the classifiers. Finally, you will be expected to answer some conceptual questions based on your observations and analysis.\n",
    "\n",
    "## Data Set:\n",
    "In this assignment, you will work with multiple versions of one dataset called \"Amphibians.\" It is adopted from a famous public dataset, and you can find more details about it <a href= \"https://archive.ics.uci.edu/ml/datasets/Amphibians#\"> here </a>.  The dataset includes information about five groups of amphibians in Poland: \"Green frogs,\" \"Brown frogs,\" \"Common toad,\" \"Tree frog,\" and \"Fire-bellied toad.\" The dataset comprises 14 attributes and one class. Some of these attributes are numeric, some are categorical, and some are ordinal.\n",
    "\n",
    "You can find details about all the features in the dataset in the file \"README.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b26164",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:48.443317Z",
     "end_time": "2023-04-05T11:25:49.349434Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa339cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:49.351433Z",
     "end_time": "2023-04-05T11:25:49.822511Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d90b111-3c9f-4b4c-a0a6-413000636fc6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:49.823512Z",
     "end_time": "2023-04-05T11:25:49.838009Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# ignore future warnings \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1c496",
   "metadata": {},
   "source": [
    "## Question 1 [5 marks]\n",
    "**Q1.1 A.** Read the dataset \"amphibians1.csv\" dataset into a pandas DataFrame called `data1`. Create a function called `encode` that takes in the features of the dataset as a pandas DataFrame and uses one-hot encoding to convert all nominal (and ordinal) attributes to numeric. You can achieve this by either using `get_dummies()` from the pandas library or `OneHotEncoder()` from the scikit-learn library. **[1 mark]**\n",
    "\n",
    "**B.** For 10 rounds, use `train_test_split` to divide the encoded `data1` into 70% train, 30% test . Set the `random_state` equal to the loop counter. For example in the loop\n",
    "``` python \n",
    "for i in range(10):\n",
    "```\n",
    "make `random_state` equal to `i`. \n",
    "Use the splitted datasets to train and test the following models (use the default hyperparameters): **[1 mark]**\n",
    "- Zero-R\n",
    "- Gaussian Naive Bayes\n",
    "- Multinomial Naive Bayes\n",
    "- Bernoulli Naive Bayes model\n",
    "\n",
    "Report the average accuracy over the 10 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c33a9d2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:49.840006Z",
     "end_time": "2023-04-05T11:25:49.857006Z"
    }
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"amphibians1.csv\")\n",
    "\n",
    "y1 = data1.iloc[:, -1]\n",
    "X1 = data1.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588459ef-a861-4793-8c92-2f57bab4bb10",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:49.856006Z",
     "end_time": "2023-04-05T11:25:49.870008Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def encode(X):\n",
    "    # your code here\n",
    "    categorical_features = ['TR', 'VR', 'SUR1', 'SUR2', 'SUR3', 'UR', 'FR', 'RR', 'BR', 'MR', 'CR']\n",
    "    X_trans = pd.get_dummies(X, columns=categorical_features)\n",
    "    return X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e3d435-af7a-4bd4-b481-f22b1c7550a2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:49.873015Z",
     "end_time": "2023-04-05T11:25:50.023415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ZeroR: 0.59\n",
      "Accuracy of GNB: 0.4\n",
      "Accuracy of MNB: 0.35\n",
      "Accuracy of BNB: 0.63\n"
     ]
    }
   ],
   "source": [
    "ZeroR_Acc_1 = []\n",
    "GNB_Acc_1 = []\n",
    "MNB_Acc_1 = []\n",
    "BNB_Acc_1 = []\n",
    "\n",
    "# your code here\n",
    "X_encode = encode(X1)\n",
    "\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encode, y1, test_size=0.30, random_state=i)\n",
    "\n",
    "    zeror = DummyClassifier(strategy=\"most_frequent\")\n",
    "    zeror.fit(X_train, y_train)\n",
    "    acc = zeror.score(X_test, y_test)\n",
    "    # print(\"ZeroR score %f \" % acc)\n",
    "    ZeroR_Acc_1.append(acc)\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(X_test, y_test)\n",
    "    # print(\"GNB score %f \" % acc)\n",
    "    GNB_Acc_1.append(acc)\n",
    "\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    # print(\"MNB score %f \" % acc)\n",
    "    MNB_Acc_1.append(acc)\n",
    "\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    # print(\"BNB score %f \" % acc)\n",
    "    BNB_Acc_1.append(acc)\n",
    "\n",
    "print(\"Accuracy of ZeroR:\", np.mean(ZeroR_Acc_1).round(2))\n",
    "print(\"Accuracy of GNB:\", np.mean(GNB_Acc_1).round(2))\n",
    "print(\"Accuracy of MNB:\", np.mean(MNB_Acc_1).round(2))\n",
    "print(\"Accuracy of BNB:\", np.mean(BNB_Acc_1).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c394c5",
   "metadata": {},
   "source": [
    "**Q1.2** After comparing the performance of the different models on the classification task, please comment on any differences or lack of differences you observe between the models. **[3 marks]**</br>\n",
    "*NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3936c2-b548-4825-8028-c5b3f3800b63",
   "metadata": {},
   "source": [
    "*Answer Here*\n",
    "\n",
    "#### Q1.2 Answer\n",
    "\n",
    "Based on the results using the `classification_report` function, here are the differences and lack of differences between the models:\n",
    "\n",
    "**Differences:**\n",
    "<1> Overall accuracy: The BNB model has the highest overall accuracy (0.72), followed by ZEROR (0.63), GNB (0.46), and MNB (0.35). This indicates that the BNB model performs better in classifying the frog species in this dataset. The possible reason for this is that the BNB model assumes that the input features are binary (i.e. 0 or 1), whereas after using one-hot encoding to convert the categorical features to binary features, those features have only 0 or 1 values, which makes the BNB model more suitable for this dataset.\n",
    "\n",
    "<2> Precision, Recall, and F1-score: The BNB model consistently has higher precision, recall, and F1-score across most classes, except for Common toad, which has a perfect score in GNB. In contrast, the ZEROR model has the lowest performance in these metrics for Brown frog, Common toad, and Tree frog classes, because it only predicts the majority class (Green frog) for all instances, without considering any feature values.\n",
    "\n",
    "<3> Presence of Fire-bellied toad: The GNB and MNB models include the Fire-bellied toad class in their results, despite having no samples in the dataset, while ZEROR and BNB do not.\n",
    "\n",
    "**Lack of Differences:**\n",
    "<1> All models show relatively poor performance when classifying Fire-bellied toad, Common toad, and Tree frog instances. This is due to the limited number of samples for these classes, which makes it difficult for the models to learn and generalize well. In this dataset, there is only 3 samples for Fire-bellied toad, 7 samples for Common toad, and 4 samples for Tree frog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_encode, y1, test_size=0.30, random_state=0)\n",
    "#\n",
    "# zeror.fit(X_train, y_train)\n",
    "# zeror_predictions = zeror.predict(X_test)\n",
    "#\n",
    "# gnb.fit(X_train, y_train)\n",
    "# gnb_predictions = gnb.predict(X_test)\n",
    "#\n",
    "# mnb.fit(X_train, y_train)\n",
    "# mnb_predictions = mnb.predict(X_test)\n",
    "#\n",
    "# bnb.fit(X_train, y_train)\n",
    "# bnb_predictions = bnb.predict(X_test)\n",
    "#\n",
    "# print(\"\\n\\n ===========\\n ZEROR FULL RESULTS\\n===========\")\n",
    "# print(classification_report(y_test, zeror_predictions))\n",
    "#\n",
    "# print(\"\\n\\n ===========\\n GNB FULL RESULTS\\n===========\")\n",
    "# print(classification_report(y_test, gnb_predictions))\n",
    "#\n",
    "# print(\"\\n\\n ===========\\n MNB FULL RESULTS\\n===========\")\n",
    "# print(classification_report(y_test, mnb_predictions))\n",
    "#\n",
    "# print(\"\\n\\n ===========\\n BNB FULL RESULTS\\n===========\")\n",
    "# print(classification_report(y_test, bnb_predictions))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:50.025414Z",
     "end_time": "2023-04-05T11:25:50.068549Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "21ec6a7b",
   "metadata": {},
   "source": [
    "## Question 2 [5 marks]\n",
    "\n",
    "**Q2.1.** Divide the `data1` into 70% train and 30% test splits for 10 rounds, set the `random_state` equal to the loop counter. Then, train and test **K-Nearest Neighbor algorithms (with K values of 1, 5, and 20)**, using Euclidean distance as the distance metric and maximum vote (no weighting) to determine the label. Finally, report the average accuracy of the KNN models over the 10 runs. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef637cf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:50.043416Z",
     "end_time": "2023-04-05T11:25:50.196413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN(1): 0.56\n",
      "Accuracy of KNN(5): 0.6\n",
      "Accuracy of KNN(20): 0.61\n"
     ]
    }
   ],
   "source": [
    "KNN1_Acc_1 = []\n",
    "KNN5_Acc_1 = []\n",
    "KNN20_Acc_1 = []\n",
    "\n",
    "# your code here\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encode, y1, test_size=0.30, random_state=i)\n",
    "\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=1, metric='euclidean', weights='uniform')\n",
    "    knn1.fit(X_train, y_train)\n",
    "    acc = knn1.score(X_test, y_test)\n",
    "    KNN1_Acc_1.append(acc)\n",
    "\n",
    "    knn5 = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "    knn5.fit(X_train, y_train)\n",
    "    acc = knn5.score(X_test, y_test)\n",
    "    KNN5_Acc_1.append(acc)\n",
    "\n",
    "    knn20 = KNeighborsClassifier(n_neighbors=20, metric='euclidean', weights='uniform')\n",
    "    knn20.fit(X_train, y_train)\n",
    "    acc = knn20.score(X_test, y_test)\n",
    "    KNN20_Acc_1.append(acc)\n",
    "\n",
    "print(\"Accuracy of KNN(1):\", np.mean(KNN1_Acc_1).round(2))\n",
    "print(\"Accuracy of KNN(5):\", np.mean(KNN5_Acc_1).round(2))\n",
    "print(\"Accuracy of KNN(20):\", np.mean(KNN20_Acc_1).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4822d",
   "metadata": {},
   "source": [
    "**Q2.2.A.** Create a function called `normalise` that takes the features of the dataset as a pandas DataFrame and scales all numeric attributes to the range of 0-1. You can either use `MinMaxScaler` from the `sklearn.preprocessing` library or implement the normalization step yourself.\n",
    "**B.** For 10 rounds divide the normalised \"data1\" into 70% train and 30% test splits using set the `random_state` equal to the loop counter, and run the KNN models (k=1,5 and 20). Report the average accuracy of your KNN models over these 10 runs. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def normalise(X):\n",
    "    # your code here\n",
    "    numeric_features = ['SR', 'NR', 'OR']\n",
    "    X_trans = X.copy()\n",
    "    for feature in numeric_features:\n",
    "        min_value = X[feature].min()\n",
    "        max_value = X[feature].max()\n",
    "        X_trans[feature] = (X[feature] - min_value) / (max_value - min_value)\n",
    "    return X_trans"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:50.199414Z",
     "end_time": "2023-04-05T11:25:50.241412Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0859a139",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:50.217416Z",
     "end_time": "2023-04-05T11:25:51.268415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN(1): 0.54\n",
      "Accuracy of KNN(5): 0.58\n",
      "Accuracy of KNN(20): 0.63\n"
     ]
    }
   ],
   "source": [
    "######################################## POSSIBLE SOLUTION #############################################\n",
    "\n",
    "KNN1_Acc_2 = []\n",
    "KNN5_Acc_2 = []\n",
    "KNN20_Acc_2 = []\n",
    "\n",
    "# your code here\n",
    "X_encode_normalise = normalise(X_encode)\n",
    "\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encode_normalise, y1, test_size=0.30, random_state=i)\n",
    "\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=1, metric='euclidean', weights='uniform')\n",
    "    knn1.fit(X_train, y_train)\n",
    "    acc = knn1.score(X_test, y_test)\n",
    "    KNN1_Acc_2.append(acc)\n",
    "\n",
    "    knn5 = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "    knn5.fit(X_train, y_train)\n",
    "    acc = knn5.score(X_test, y_test)\n",
    "    KNN5_Acc_2.append(acc)\n",
    "\n",
    "    knn20 = KNeighborsClassifier(n_neighbors=20, metric='euclidean', weights='uniform')\n",
    "    knn20.fit(X_train, y_train)\n",
    "    acc = knn20.score(X_test, y_test)\n",
    "    KNN20_Acc_2.append(acc)\n",
    "\n",
    "print(\"Accuracy of KNN(1):\", np.mean(KNN1_Acc_2).round(2))\n",
    "print(\"Accuracy of KNN(5):\", np.mean(KNN5_Acc_2).round(2))\n",
    "print(\"Accuracy of KNN(20):\", np.mean(KNN20_Acc_2).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357cd59",
   "metadata": {},
   "source": [
    "**Q2.3** Compare the results of the KNN models (for each value of K) in Q2.1 and Q2.2, and discuss any differences you observe. Did the preprocessing step in Q2.2 improve the performance of the KNN models? Why or why not? **[3 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc7597",
   "metadata": {},
   "source": [
    "*Answer Here*\n",
    "\n",
    "#### Q2.3 Answer\n",
    "KNN(1): The accuracy slightly decreases from 0.56 before normalization to 0.54 after normalization.\n",
    "\n",
    "KNN(5): The accuracy also slightly decreases from 0.6 before normalization to 0.58 after normalization.\n",
    "\n",
    "KNN(20): The accuracy increases from 0.61 before normalization to 0.63 after normalization.\n",
    "\n",
    "Based on these results, the preprocessing step (normalization) in Q2.2 does not consistently improve the performance of the KNN models for k=1 and k=5. However, the performance of the KNN(20) model does improve after normalization.\n",
    "\n",
    "Normalization can improve the performance of KNN models because it scales numerical features to the range of 0-1, preventing features with larger magnitudes (e.g. SR, which mainly ranges from -50 to 8000) from dominating the distance calculations. However, in this case, the improvement is not consistent across different values of k. This could be due to the fact that the dataset is small, and the features are not very different in magnitude. Therefore, the normalization step does not have a significant impact on the performance of the KNN models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533050f",
   "metadata": {},
   "source": [
    "## Question 3 [5 marks]\n",
    "\n",
    "**Q3.1.** Read the \"amphibians2.csv\" into `data2` and  \"amphibians1.csv\" into `data1_2`. Observe any differences in the values of the feature \"SR\" between these two datasets, and explain why you think these changes were made (provide your hypothesis). Use diagrams such a histograms or boxplots to check the distribution of the feature 'SR' before and after the change. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf383bfa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:51.269415Z",
     "end_time": "2023-04-05T11:25:51.313413Z"
    }
   },
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"amphibians2.csv\")\n",
    "data1_2 = pd.read_csv(\"amphibians1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93740fd5-3ec4-4b50-8d0c-def8367fa0ed",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:51.285419Z",
     "end_time": "2023-04-05T11:25:51.423416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "data1_2['SR'].hist(color='blue', alpha=0.5)\n",
    "data2['SR'].hist(color='yellow', alpha=0.5)\n",
    "# print(data1_2['SR'].value_counts(bins=10))\n",
    "# print(data2['SR'].value_counts(bins=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfddfd-cc6b-47d7-9d26-a3d699ac5d10",
   "metadata": {},
   "source": [
    "*Answer Here*\n",
    "\n",
    "#### Q3.1 Answer\n",
    "Upon observation, it was found that in \"amphibians2.csv\" (data2), the values of the feature \"SR\" are more compact, with most of the data concentrated in the 0-3,000 range. This is in contrast to \"amphibians1.csv\" (data1_2), where most of the values range from -50 to 8,000. The following are my hypotheses for the changes made to the \"SR\" feature:\n",
    "\n",
    "<1> Since the values of the feature \"SR\" were observed to be much larger than those of the other features, compressing the value range of the \"SR\" feature might have been done to reduce the scale differences between the features. When features have different scales, machine learning models may assign excessive weight to larger-valued features, which can negatively impact model performance. By reducing the value range of the \"SR\" feature, a more balanced data handling by the model can be ensured.\n",
    "\n",
    "<2> In data1_2, the \"SR\" feature contains several negative values, which should be positive since they represent the reservoir's surface area in square meters. In addition, the extreme values in the \"SR\" feature (i.e. values greater than 8,000) could also have an impact on the model's performance. By removing the negative and extreme large values, the model's performance can be improved. A potential approach to compress the data could involve converting the area units from square meters to larger area units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec583024",
   "metadata": {},
   "source": [
    "**Q3.2** \n",
    "For 10 rounds split the `data2` and `data1_2` dataset into 70% training and 30% testing sets use the `random_state` equal to the loop counter. Train and test two **decision tree** classifier with all default hyper-parameters: one with `data1_2` and one with `data2`. Calculate and report the average accuracy of both models for 10 runs. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fca15dcd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:51.425417Z",
     "end_time": "2023-04-05T11:25:51.463413Z"
    }
   },
   "outputs": [],
   "source": [
    "y1_2 = data1_2.iloc[:, -1]\n",
    "X1_2 = data1_2.iloc[:, :-1]\n",
    "\n",
    "y2 = data2.iloc[:, -1]\n",
    "X2 = data2.iloc[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18064b7e-be1e-40ba-9197-100b8c236ce9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:51.441415Z",
     "end_time": "2023-04-05T11:25:51.501413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree \tdata1_2: 0.53 \tdata2: 0.53\n"
     ]
    }
   ],
   "source": [
    "DT_Acc_1_2 = []\n",
    "DT_Acc_2 = []\n",
    "\n",
    "# your code here\n",
    "for i in range(10):\n",
    "    X_train1_2, X_test1_2, y_train1_2, y_test1_2 = train_test_split(X1_2, y1_2, test_size=0.3, random_state=i)\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=i)\n",
    "\n",
    "    DT1_2 = DecisionTreeClassifier(random_state=0)\n",
    "    DT1_2.fit(X_train1_2, y_train1_2)\n",
    "    acc = DT1_2.score(X_test1_2, y_test1_2)\n",
    "    DT_Acc_1_2.append(acc)\n",
    "\n",
    "    DT2 = DecisionTreeClassifier(random_state=0)\n",
    "    DT2.fit(X_train2, y_train2)\n",
    "    acc = DT2.score(X_test2, y_test2)\n",
    "    DT_Acc_2.append(acc)\n",
    "\n",
    "print(\"Accuracy of Decision Tree \\tdata1_2:\", np.mean(DT_Acc_1_2).round(2), \"\\tdata2:\", np.mean(DT_Acc_2).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82dd7fb",
   "metadata": {},
   "source": [
    "**Q3.3** Compare and analyze the performance of the **decision tree** classifier on `data1_2` and `data2`. Discuss any differences or similarities that you observe in the performance of these models. Does the change made to the dataset improve the performance of the model? Explain why or why not and elaborate on your hypothesis from Q3.1. **[2 marks]** </br>*NOTE:  You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e6985",
   "metadata": {},
   "source": [
    "*Answer Here*\n",
    "\n",
    "#### Q3.3 Answer\n",
    "\n",
    "In comparing the performance of the decision tree classifier on data1_2 and data2, it is observed that the average accuracy is 0.53 for both datasets. This indicates that the model's performance is similar on both datasets. Based on this result, it can be inferred that the changes made to the dataset did not improve the model's performance.\n",
    "\n",
    "Observing the classification_report, there are some differences in the model's performance on the two datasets. On data1_2, the model has a higher accuracy for Green frogs (0.80), while on data2, the model has a higher accuracy for Tree frogs (0.50). Additionally, on data1_2, recall, F1 score, and weighted average accuracy are higher, whereas on data2, macro average accuracy and precision are higher.\n",
    "\n",
    "According to the hypothesis formulated in Q3.1, it was expected that compressing the \"SR\" feature would have a positive impact on the model's performance. However, in practice, this change did not improve the model's performance. This could be because decision tree classifiers are not sensitive to the different scales of features, and as a result, the compression of the \"SR\" feature may have limited impact on the performance of this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# X_train1_2, X_test1_2, y_train1_2, y_test1_2 = train_test_split(X1_2, y1_2, test_size=0.3, random_state=0)\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=0)\n",
    "#\n",
    "# DT1_2.fit(X_train1_2, y_train1_2)\n",
    "# DT1_2_pred = DT1_2.predict(X_test1_2)\n",
    "#\n",
    "# DT2.fit(X_train2, y_train2)\n",
    "# DT2_pred = DT2.predict(X_test2)\n",
    "#\n",
    "# print(classification_report(y_test1_2, DT1_2_pred))\n",
    "# print(classification_report(y_test2, DT2_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:51.502414Z",
     "end_time": "2023-04-05T11:25:51.518414Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "dc2419db",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "**Q4.1** Read the \"amphibians3.csv\" into data3. Use histogram diagrams to compare the distribution of class labels between `data1` and `data3`. Observe the changes on the distribution of the class labels and explain in your own words why you think these changes have been made.**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ad0ce1b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:51.520416Z",
     "end_time": "2023-04-05T11:25:51.657418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1152x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data3 = pd.read_csv(\"amphibians3.csv\")\n",
    "\n",
    "# your code here\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.hist(data1['frog class'], alpha=0.5, color='blue')\n",
    "ax1.set_ylim(0, 120)\n",
    "ax1.set_xlabel('Data1')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.hist(data3['frog class'], alpha=0.5, color='red')\n",
    "ax2.set_ylim(0, 120)\n",
    "ax2.set_xlabel('Data3')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "# print(data1['frog class'].value_counts())\n",
    "# print(data3['frog class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81f752",
   "metadata": {},
   "source": [
    "**Q4.2** Use the same encoding technique as used in Q1.1 to encode the `data3` dataset and apply the same normalization technique as used in Q2.1 to normalise the data. For 10 rounds, split the encoded and normalized data3 into 70% training and 30% testing sets,  set the `random_state` equal to the loop counter. Train and test the following models using `data3`: **[1 mark]**\n",
    "- Zero-R\n",
    "- K-Nearest Neighbour (K = 1, 5 and 20) \n",
    "\n",
    "Calculate the average accuracy of the models for 10 runs and report the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9e5ba46",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:51.658420Z",
     "end_time": "2023-04-05T11:25:51.725413Z"
    }
   },
   "outputs": [],
   "source": [
    "y3 = data3.iloc[:, -1]\n",
    "X3 = data3.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b9fad1b-d2fc-4adc-818b-9cee1c204bc7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-05T11:25:51.677414Z",
     "end_time": "2023-04-05T11:25:52.632993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ZeroR: \tdata1: 0.59 \tdata3: 0.14\n",
      "Accuracy of KNN(1): \tdata1: 0.54 \tdata3: 0.76\n",
      "Accuracy of KNN(5): \tdata1: 0.58 \tdata3: 0.65\n",
      "Accuracy of KNN(20): \tdata1: 0.63 \tdata3: 0.44\n"
     ]
    }
   ],
   "source": [
    "ZeroR_Acc_3 = []\n",
    "KNN1_Acc_3 = []\n",
    "KNN5_Acc_3 = []\n",
    "KNN20_Acc_3 = []\n",
    "\n",
    "# your code here\n",
    "X3_encode = encode(X3)\n",
    "X3_encode_normalise = normalise(X3_encode)\n",
    "\n",
    "for i in range(10):\n",
    "    X_train3, X_test3, y_train3, y_test3 = train_test_split(X3_encode_normalise, y3, test_size=0.3, random_state=i)\n",
    "\n",
    "    ZeroR = DummyClassifier(strategy='most_frequent')\n",
    "    ZeroR.fit(X_train3, y_train3)\n",
    "    acc = ZeroR.score(X_test3, y_test3)\n",
    "    ZeroR_Acc_3.append(acc)\n",
    "\n",
    "    KNN1 = KNeighborsClassifier(n_neighbors=1)\n",
    "    KNN1.fit(X_train3, y_train3)\n",
    "    acc = KNN1.score(X_test3, y_test3)\n",
    "    KNN1_Acc_3.append(acc)\n",
    "\n",
    "    KNN5 = KNeighborsClassifier(n_neighbors=5)\n",
    "    KNN5.fit(X_train3, y_train3)\n",
    "    acc = KNN5.score(X_test3, y_test3)\n",
    "    KNN5_Acc_3.append(acc)\n",
    "\n",
    "    KNN20 = KNeighborsClassifier(n_neighbors=20)\n",
    "    KNN20.fit(X_train3, y_train3)\n",
    "    acc = KNN20.score(X_test3, y_test3)\n",
    "    KNN20_Acc_3.append(acc)\n",
    "\n",
    "print(\"Accuracy of ZeroR: \\tdata1:\", np.mean(ZeroR_Acc_1).round(2), \"\\tdata3:\", np.mean(ZeroR_Acc_3).round(2))\n",
    "print(\"Accuracy of KNN(1): \\tdata1:\", np.mean(KNN1_Acc_2).round(2), \"\\tdata3:\", np.mean(KNN1_Acc_3).round(2))\n",
    "print(\"Accuracy of KNN(5): \\tdata1:\", np.mean(KNN5_Acc_2).round(2), \"\\tdata3:\", np.mean(KNN5_Acc_3).round(2))\n",
    "print(\"Accuracy of KNN(20): \\tdata1:\", np.mean(KNN20_Acc_2).round(2), \"\\tdata3:\", np.mean(KNN20_Acc_3).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4249859",
   "metadata": {},
   "source": [
    "**Q4.3** Discuss any differences you observe between the results of these KNN classifiers on data1 (Q2.2) and data3. **[2 marks]** </br>*NOTE:  You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7067969a",
   "metadata": {},
   "source": [
    "*Answer Here*\n",
    "\n",
    "#### Q4.3 Answer\n",
    "\n",
    "Here are some of the differences between the results of the KNN classifiers on data1 and data3:\n",
    "\n",
    "<1> Accuracy of ZeroR: The ZeroR classifier, which predicts the majority class, has an accuracy of 0.59 on data1 and 0.14 on data3. This is expected since data1 is imbalanced, and the majority class constitutes a significant portion of the dataset. In data3, the class distribution is balanced, so the ZeroR classifier's accuracy is approximately 1 divided by the number of classes (1/5 = 0.2), which is closer to the observed 0.14.\n",
    "\n",
    "<2> Accuracy of KNN: In the KNN classifier (K=1, 5, 20), the accuracy of data3 is higher than data1 for K=1 and K=5, while for K=20, the accuracy of data1 is higher than data3. Therefore, the overall performance is improved, probably because the interference of the sample corresponding to the dominant label (Green frog) has been reduced.\n",
    "\n",
    "<3> Effect of K: As the value of K increases, the accuracy of the KNN classifier on data1 improves (from 0.54 to 0.63), while it decreases on data3 (from 0.76 to 0.44). This may suggest that, in data1, a larger K value helps to mitigate the impact of class imbalance and noise, while in data3, a smaller K value is more suitable due to the balanced class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55940e72-3414-4dab-be69-96a579354617",
   "metadata": {},
   "source": [
    "# Authorship Declaration:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: Jiahao Shen 1381187\n",
    "   \n",
    "   <b>Dated</b>: 04/04/2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
