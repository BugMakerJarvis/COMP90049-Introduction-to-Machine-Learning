{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP90049 Introduction to Machine Learning, 2023 Semester 1\n",
    "\n",
    "## Week 4 - Sample Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will be using `scikit-learn` to classify some data, and to evaluate some classifiers.\n",
    "\n",
    "Scikit-learn is a popular Python library for machine learning that provides a wide range of tools for data preprocessing, modeling, and evaluation. It is built on top of NumPy, SciPy, and Matplotlib, and is designed to be easy to use and efficient for both small and large-scale machine learning tasks.\n",
    "\n",
    "Scikit-learn includes a variety of machine learning algorithms, such as classification, regression, clustering, and dimensionality reduction, and provides a consistent API for using these algorithms. It also includes tools for feature extraction, selection, and scaling, as well as for evaluating and optimizing machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "Please load Car Evaluation dataset from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data).\n",
    "\n",
    "The common terminology in scikit-learn is that the array defining the attribute values is called X and the array defining the gold–standard (“ground truth”) labels is called y ; create these variables for the car data.\n",
    "\n",
    "- **(a)** Load the data into a suitable format for scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('car.data', header = None)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** How many instances are there in this collection? How many attributes, and of what type(s)? What is the class we’re trying to predict, and how many values does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1728 instances\n",
      "There are 6 attributes\n",
      "There are 4 class labels: ['acc' 'good' 'unacc' 'vgood']\n",
      "Label frequencies: [('acc', 384), ('good', 69), ('unacc', 1210), ('vgood', 65)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('There are', len(X), 'instances')\n",
    "print('There are', len(X.columns), \"attributes\")\n",
    "\n",
    "unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print('There are', len(unique_labels), \"class labels:\", unique_labels)   \n",
    "#use Counter to count the number of labels\n",
    "label_counter = Counter(y)\n",
    "print(\"Label frequencies:\", list(zip(unique_labels, label_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Unfortunately, scikit-learn isn’t set up to deal with our attributes in this format.\n",
    "\n",
    "- **(a)** Write some functions that transform our **categorical** attributes into **numerical** attributes, by (perhaps arbitrarily) assigning each categorical value to an integer, for example:\n",
    "\n",
    "\n",
    "\"unacc\" = 0\n",
    "\"acc\" = 1\n",
    "\"good\" = 2\n",
    "\"vgood\" = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for feature 0 that values are ['high' 'low' 'med' 'vhigh']\n",
      "for feature 1 that values are ['high' 'low' 'med' 'vhigh']\n",
      "for feature 2 that values are ['2' '3' '4' '5more']\n",
      "for feature 3 that values are ['2' '4' 'more']\n",
      "for feature 4 that values are ['big' 'med' 'small']\n",
      "for feature 5 that values are ['high' 'low' 'med']\n"
     ]
    }
   ],
   "source": [
    "# Here's one  way of reading this from the data itself\n",
    "for column in X:\n",
    "    print(\"for feature\", column, \"that values are\", np.unique(X[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X.apply(lambda x: x.astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`This code converts each column of a pandas DataFrame 'X' containing numerical data into a categorical data type using 'astype('category')', and then converts the categorical data into numeric codes using the 'cat.codes' method. The resulting DataFrame 'X_num' will have the same shape as the original DataFrame 'X', but with each numerical column replaced by a categorical column with corresponding numeric codes.`\n",
    "\n",
    "`The lambda function 'lambda x: x.astype('category').cat.codes' is used with the 'apply()' method to apply this conversion to each column of the DataFrame 'X'. The 'apply()' method applies a function to each column or row of a DataFrame, and returns a new DataFrame with the same shape as the original DataFrame.`\n",
    "\n",
    "`In Python, a lambda function is a small anonymous function that can take any number of arguments but can only have one expression. It is also known as an \"inline function\" or a \"function object.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for feature 0 that values are [0 1 2 3]\n",
      "for feature 1 that values are [0 1 2 3]\n",
      "for feature 2 that values are [0 1 2 3]\n",
      "for feature 3 that values are [0 1 2]\n",
      "for feature 4 that values are [0 1 2]\n",
      "for feature 5 that values are [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "for column in X_num:\n",
    "    print(\"for feature\", column, \"that values are\", np.unique(X_num[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Split the data into training (80%) and test sets (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1157, 6) X_test: (571, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print('X_train: {} X_test: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "Read up on different implementations of the Naive Bayes classifier in `sklearn.naive_bayes`. Which one do you think is most suitable for the dataset we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Compare the accuracies of all three different kinds of Naive Bayes classifier. Does this accord with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GNB score 0.607706 \n",
      "MNB score 0.698774 \n",
      "BNB score 0.754816 \n",
      "\n",
      "GNB score 0.653240 \n",
      "MNB score 0.712785 \n",
      "BNB score 0.761821 \n",
      "\n",
      "GNB score 0.637478 \n",
      "MNB score 0.714536 \n",
      "BNB score 0.746060 \n",
      "\n",
      "Avg GNB score: 0.6328079392877992\n",
      "Avg MNB score: 0.7086981903093986\n",
      "Avg BNB score: 0.7542323409223585\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes as nb\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_num, y, test_size=0.33, random_state=i)\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(X_test, y_test)\n",
    "    print(\"\\nGNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('\\nAvg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We might have expected that Gaussian NB would work a little bit better here, but the ordering appears to be less significant than the feature values themselves. A secondary concern might the uniform distribution of attribute values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** By default, this implementation of Naive Bayes uses Laplace smoothing. Turn this off, and see what happens — what is the significance of the reported accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB score 0.714536 \n",
      "BNB score 0.742557 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda-navigator\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "F:\\Anaconda-navigator\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "# Gaussian NB doesn't use smoothing; all of the probabilities for the Gaussian are already non-zero\n",
    "# You can try this for yourself, but scikit-learn will flatly refuse to do it\n",
    "mnb = MultinomialNB(alpha=0)\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "mnb.fit(X_train, y_train)\n",
    "acc = mnb.score(X_test, y_test)\n",
    "print(\"MNB score %f \" %acc)\n",
    "    \n",
    "    \n",
    "bnb.fit(X_train, y_train)\n",
    "acc = bnb.score(X_test, y_test)\n",
    "print(\"BNB score %f \" %acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due to the implementation (as log-probabilities), numerical errors would result from unseen events.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** What happens if we change the smoothing parameter ($\\alpha$)? Calculate the accuracy for a range of values from 5 to 500. For the very large values, examine the predicted classes for the test instances — what is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MNB with aplha = 5  score is 0.716287 \n",
      "BNB with aplha = 5  score is 0.746060 \n",
      "\n",
      "MNB with aplha = 20  score is 0.714536 \n",
      "BNB with aplha = 20  score is 0.746060 \n",
      "\n",
      "MNB with aplha = 100  score is 0.714536 \n",
      "BNB with aplha = 100  score is 0.718039 \n",
      "\n",
      "MNB with aplha = 500  score is 0.714536 \n",
      "BNB with aplha = 500  score is 0.714536 \n"
     ]
    }
   ],
   "source": [
    "Alpha_list = [5,20,100,500]\n",
    "\n",
    "for i in Alpha_list:\n",
    "    mnb = MultinomialNB(alpha=i)\n",
    "    bnb = BernoulliNB(alpha=i)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"\\nMNB with aplha =\", i ,\" score is %f \" %acc)\n",
    "\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB with aplha =\", i ,\" score is %f \" %acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`For large values of the smoothing parameter, every instance is predicted to be the majority-class - effectively, this is the same behaviour as 0-R!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.\n",
    "The transformation of the data in Q2 implicitly creates ordinal attributes. At first glance, such a strategy does seem reasonable in light of the given values (such as *small, med, big*).\n",
    "A different strategy would be to `binarise` the attributes: to replace a categorical attribute having `m` values with `m binary attributes`. One way of doing this in scikit-learn is using the **OneHotEncoder** :\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "X_trans = ohe.transform(X).toarray()\n",
    "```\n",
    "\n",
    "Note that this transformation should be done before we split the data into training and test sets. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Check the shape of `X_trans` — how many attributes do we have now? Does this correspond to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728, 21)\n",
      "X: 0    vhigh\n",
      "1    vhigh\n",
      "2        2\n",
      "3        2\n",
      "4    small\n",
      "5      low\n",
      "Name: 0, dtype: object\n",
      "X_trans: 0     0.0\n",
      "1     0.0\n",
      "2     0.0\n",
      "3     1.0\n",
      "4     0.0\n",
      "5     0.0\n",
      "6     0.0\n",
      "7     1.0\n",
      "8     1.0\n",
      "9     0.0\n",
      "10    0.0\n",
      "11    0.0\n",
      "12    1.0\n",
      "13    0.0\n",
      "14    0.0\n",
      "15    0.0\n",
      "16    0.0\n",
      "17    1.0\n",
      "18    0.0\n",
      "19    1.0\n",
      "20    0.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "X_trans_ohe = ohe.transform(X).toarray()\n",
    "X_trans = pd.DataFrame(X_trans_ohe)\n",
    "\n",
    "print(X_trans.shape)\n",
    "print('X:', X.iloc[0])\n",
    "print('X_trans:', X_trans.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Split the dataset comprised of `one–hot attributes` into **train** and **test** sets. Compare the accuracies of the three Naive Bayes models using ordinal attributes with the three models using `one–hot attributes`: are you surprised? What can we infer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GNB score 0.793345 \n",
      "MNB score 0.816112 \n",
      "BNB score 0.837128 \n",
      "\n",
      "GNB score 0.824869 \n",
      "MNB score 0.865149 \n",
      "BNB score 0.891419 \n",
      "\n",
      "GNB score 0.789842 \n",
      "MNB score 0.814361 \n",
      "BNB score 0.858144 \n",
      "\n",
      "Avg GNB score: 0.8026853473438411\n",
      "Avg MNB score: 0.8318739054290717\n",
      "Avg BNB score: 0.8622300058377116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size=0.33, random_state=i)\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(X_test, y_test)\n",
    "    print(\"\\nGNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('\\nAvg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`This is a fairly drastic difference: Bernoulli NB is still the best option, but both Gaussian and Multinomial NB are no longer useless. It appears that all of these learners can identify meaningful patterns, just by taking the attribute value in isolation (and not in relation to the presumed ordering) - and so, perhaps our original assignment of 0,1,2,3 was too simple to discover patterns.`\n",
    "\n",
    "`At this point, we can also observe that the default behaviour of scikit-learn's Bernoulli NB is to do ... something ... with non-binary attributes, but it is usually better to make them explicitly binary using the one-hot transformer. (If you're curious, in this case, it's treating whichever value is 0 as \"N\", and the other values as \"Y\".)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.\n",
    "\n",
    "Now let's check other metrics results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ===========\n",
      " MNB FULL RESULTS\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       0.57      0.57      0.57       121\n",
      "        good       0.50      0.21      0.29        24\n",
      "       unacc       0.89      0.95      0.92       408\n",
      "       vgood       1.00      0.22      0.36        18\n",
      "\n",
      "    accuracy                           0.81       571\n",
      "   macro avg       0.74      0.49      0.54       571\n",
      "weighted avg       0.81      0.81      0.80       571\n",
      "\n",
      "\n",
      "\n",
      " ===========\n",
      " BNB FULL RESULTS\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       0.65      0.78      0.71       121\n",
      "        good       0.62      0.62      0.62        24\n",
      "       unacc       0.95      0.91      0.93       408\n",
      "       vgood       0.92      0.61      0.73        18\n",
      "\n",
      "    accuracy                           0.86       571\n",
      "   macro avg       0.79      0.73      0.75       571\n",
      "weighted avg       0.87      0.86      0.86       571\n",
      "\n",
      "\n",
      "\n",
      " ===========\n",
      " GNB FULL RESULTS\n",
      "===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       0.52      0.81      0.63       121\n",
      "        good       0.53      0.88      0.66        24\n",
      "       unacc       1.00      0.77      0.87       408\n",
      "       vgood       0.67      1.00      0.80        18\n",
      "\n",
      "    accuracy                           0.79       571\n",
      "   macro avg       0.68      0.86      0.74       571\n",
      "weighted avg       0.87      0.79      0.81       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_predictions = gnb.predict(X_test)\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "mnb_predictions = mnb.predict(X_test)\n",
    "\n",
    "bnb.fit(X_train, y_train)\n",
    "bnb_predictions = bnb.predict(X_test)\n",
    "\n",
    "print(\"\\n\\n ===========\\n MNB FULL RESULTS\\n===========\")\n",
    "print(classification_report(y_test,mnb_predictions))\n",
    "\n",
    "print(\"\\n\\n ===========\\n BNB FULL RESULTS\\n===========\")\n",
    "print(classification_report(y_test,bnb_predictions))\n",
    "\n",
    "print(\"\\n\\n ===========\\n GNB FULL RESULTS\\n===========\")\n",
    "print(classification_report(y_test,gnb_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** What is the difference between macro average and weighted average? Why the results are different for Bernoulli, Multinominal and Gaussian Naive Bayes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`In multi-class classification problems, macro average and weighted average are two commonly used methods to calculate the overall performance metrics of a classifier.`\n",
    "\n",
    "`Macro average: This method calculates the average performance metrics across all classes, without taking into account the class imbalance. For example, if there are 3 classes and the precision of the classifier for each class is 0.6, 0.7 and 0.8, the macro-averaged precision would be (0.6 + 0.7 + 0.8) / 3 = 0.7.`\n",
    "\n",
    "`Weighted average: This method calculates the average performance metrics across all classes, taking into account the class imbalance. The weight of each class is proportional to its frequency in the dataset. For example, if there are 3 classes and the number of instances of each class is 100, 200 and 300, the weighted average precision would be (0.6 * 100 + 0.7 * 200 + 0.8 * 300) / (100 + 200 + 300) = 0.733.`\n",
    "\n",
    "`The difference between macro average and weighted average lies in the way they handle class imbalance. If the dataset is balanced, i.e. all classes have roughly the same number of instances, then the macro and weighted averages will be similar. However, if the dataset is imbalanced, i.e. some classes have many more instances than others, the weighted average will be more influenced by the performance of the dominant classes, while the macro average will treat all classes equally.`\n",
    "\n",
    "`The results for Bernoulli, Multinomial, and Gaussian Naive Bayes can be different because these models have different assumptions about the distribution of the input features.`\n",
    "\n",
    "`Bernoulli Naive Bayes assumes that the input features are binary (i.e. 0 or 1), and therefore works well for text classification problems where each feature represents the presence or absence of a word.`\n",
    "\n",
    "`Multinomial Naive Bayes assumes that the input features are discrete counts, and therefore works well for classification problems where each feature represents a frequency.`\n",
    "\n",
    "`Gaussian Naive Bayes assumes that the input features are continuous and normally distributed, and therefore works well for numerical data with a Gaussian distribution.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** In this dataset which label has the best F1-score along all different NB models? Can explain the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The label with the highest F1-score is \"unacc\" due to its high frequency among the available labels in the dataset. Since a majority of the instances in the dataset are labeled as \"unacc\", the model has been trained well to accurately predict this label for most instances, leading to a higher F1-score for this label.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Question\n",
    "In this dataset which label has the worst F1-score along all different NB models? Can explain the reason."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
