{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP90049 Introduction to Machine Learning, 2023 Semester 1\n",
    "\n",
    "## Week 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will be using `scikit-learn` to classify some data, and to evaluate some classifiers.\n",
    "\n",
    "Scikit-learn is a popular Python library for machine learning that provides a wide range of tools for data preprocessing, modeling, and evaluation. It is built on top of NumPy, SciPy, and Matplotlib, and is designed to be easy to use and efficient for both small and large-scale machine learning tasks.\n",
    "\n",
    "Scikit-learn includes a variety of machine learning algorithms, such as classification, regression, clustering, and dimensionality reduction, and provides a consistent API for using these algorithms. It also includes tools for feature extraction, selection, and scaling, as well as for evaluating and optimizing machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "Please load Car Evaluation dataset from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data).\n",
    "\n",
    "The common terminology in scikit-learn is that the array defining the attribute values is called X and the array defining the gold–standard (“ground truth”) labels is called y ; create these variables for the car data.\n",
    "\n",
    "- **(a)** Load the data into a suitable format for scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('car.data', header = None)\n",
    "X = ...\n",
    "y = data.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** How many instances are there in this collection? How many attributes, and of what type(s)? What is the class we’re trying to predict, and how many values does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are Ellipsis instances\n",
      "There are Ellipsis attributes\n",
      "There are 4 class labels: ['acc' 'good' 'unacc' 'vgood']\n",
      "Label frequencies: [('acc', 384), ('good', 69), ('unacc', 1210), ('vgood', 65)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('There are', ..., 'instances')\n",
    "print('There are', ..., \"attributes\")\n",
    "\n",
    "unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print('There are', len(unique_labels), \"class labels:\", unique_labels)   \n",
    "#use Counter to count the number of labels\n",
    "label_counter = Counter(y)\n",
    "print(\"Label frequencies:\", list(zip(unique_labels, label_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Unfortunately, scikit-learn isn’t set up to deal with our attributes in this format.\n",
    "\n",
    "- **(a)** Write some functions that transform our **categorical** attributes into **numerical** attributes, by (perhaps arbitrarily) assigning each categorical value to an integer, for example:\n",
    "\n",
    "\n",
    "\"unacc\" = 0\n",
    "\"acc\" = 1\n",
    "\"good\" = 2\n",
    "\"vgood\" = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Here's one  way of reading this from the data itself\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m column \u001B[38;5;129;01min\u001B[39;00m X:\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor feature\u001B[39m\u001B[38;5;124m\"\u001B[39m, column, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthat values are\u001B[39m\u001B[38;5;124m\"\u001B[39m, np\u001B[38;5;241m.\u001B[39munique(X[column]))\n",
      "\u001B[1;31mTypeError\u001B[0m: 'ellipsis' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Here's one  way of reading this from the data itself\n",
    "for column in X:\n",
    "    print(\"for feature\", column, \"that values are\", np.unique(X[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X_num:\n",
    "    print(\"for feature\", column, \"that values are\", np.unique(X_num[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Split the data into training (80%) and test sets (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print('X_train: {} X_test: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "Read up on different implementations of the Naive Bayes classifier in `sklearn.naive_bayes`. Which one do you think is most suitable for the dataset we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Compare the accuracies of all three different kinds of Naive Bayes classifier. Does this accord with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes as nb\n",
    "##print(dir(nb))\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_num, y, test_size=0.33, random_state=i)\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(X_test, y_test)\n",
    "    print(\"\\nGNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('\\nAvg GNB score:', ...)\n",
    "print('Avg MNB score:', ...)\n",
    "print('Avg BNB score:', ...)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** By default, this implementation of Naive Bayes uses Laplace smoothing. Turn this off, and see what happens — what is the significance of the reported accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "# Gaussian NB doesn't use smoothing; all of the probabilities for the Gaussian are already non-zero\n",
    "# You can try this for yourself, but scikit-learn will flatly refuse to do it\n",
    "mnb = MultinomialNB(alpha=0)\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "mnb.fit(X_train, y_train)\n",
    "acc = mnb.score(X_test, y_test)\n",
    "print(\"MNB score %f \" %acc)\n",
    "    \n",
    "    \n",
    "bnb.fit(X_train, y_train)\n",
    "acc = bnb.score(X_test, y_test)\n",
    "print(\"BNB score %f \" %acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due to the implementation (as log-probabilities), numerical errors would result from unseen events.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(c)** What happens if we change the smoothing parameter ($\\alpha$)? Calculate the accuracy for a range of values from 5 to 500. For the very large values, examine the predicted classes for the test instances — what is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha_list = [...]\n",
    "\n",
    "for i in Alpha_list:\n",
    "    mnb = MultinomialNB(alpha=i)\n",
    "    bnb = BernoulliNB(alpha=i)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"\\nMNB with aplha =\", i ,\" score is %f \" %acc)\n",
    "\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB with aplha =\", i ,\" score is %f \" %acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.\n",
    "The transformation of the data in Q2 implicitly creates ordinal attributes. At first glance, such a strategy does seem reasonable in light of the given values (such as *small, med, big*).\n",
    "A different strategy would be to `binarise` the attributes: to replace a categorical attribute having `m` values with `m binary attributes`. One way of doing this in scikit-learn is using the **OneHotEncoder** :\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "X_trans = ohe.transform(X).toarray()\n",
    "```\n",
    "\n",
    "Note that this transformation should be done before we split the data into training and test sets. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** Check the shape of `X_trans` — how many attributes do we have now? Does this correspond to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "X_trans_ohe = ohe.transform(X).toarray()\n",
    "X_trans = pd.DataFrame(X_trans_ohe)\n",
    "\n",
    "print(X_trans.shape)\n",
    "print('X:', X.iloc[0])\n",
    "print('X_trans:', X_trans.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** Split the dataset comprised of `one–hot attributes` into **train** and **test** sets. Compare the accuracies of the three Naive Bayes models using ordinal attributes with the three models using `one–hot attributes`: are you surprised? What can we infer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size=0.33, random_state=i)\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(X_test, y_test)\n",
    "    print(\"\\nGNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('\\nAvg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a fairly drastic difference: Bernoulli NB is still the best option, but both Gaussian and Multinomial NB are no longer useless. It appears that all of these learners can identify meaningful patterns, just by taking the attribute value in isolation (and not in relation to the presumed ordering) - and so, perhaps our original assignment of 0,1,2,3 was too simple to discover patterns.*\n",
    "\n",
    "*At this point, we can also observe that the default behaviour of scikit-learn's Bernoulli NB is to do ... something ... with non-binary attributes, but it is usually better to make them explicitly binary using the one-hot transformer. (If you're curious, in this case, it's treating whichever value is 0 as \"N\", and the other values as \"Y\".)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.\n",
    "\n",
    "Now let's check other metrics results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_predictions = gnb.predict(X_test)\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "mnb_predictions = mnb.predict(X_test)\n",
    "\n",
    "bnb.fit(X_train, y_train)\n",
    "bnb_predictions = bnb.predict(X_test)\n",
    "\n",
    "print(\"\\n\\n ===========\\n MNB FULL RESULTS\\n===========\")\n",
    "print(classification_report(y_test,mnb_predictions))\n",
    "\n",
    "print(\"\\n\\n ===========\\n BNB FULL RESULTS\\n===========\")\n",
    "print(classification_report(y_test,bnb_predictions))\n",
    "\n",
    "print(\"\\n\\n ===========\\n GNB FULL RESULTS\\n===========\")\n",
    "print(classification_report(y_test,gnb_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(a)** What is the difference between macro average and weighted average? Why the results are different for Bernoulli, Multinominal and Gaussian Naive Bayes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(b)** In this dataset which label has the best F1-score along all different NB models? Can explain the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Question\n",
    "In this dataset which label has the worst F1-score along all different NB models? Can explain the reason."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
